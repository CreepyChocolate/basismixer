{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f32924b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os.path\n",
    "import partitura as pt\n",
    "from basismixer.performance_codec import get_performance_codec\n",
    "from matplotlib import pyplot as plt\n",
    "import glob\n",
    "import re\n",
    "from scipy import stats\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489bc6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_fn = glob.glob(os.path.join(\"asap-dataset\", \"**\", \"*.musicxml\"), recursive=True)\n",
    "dump_json = False\n",
    "perf_parameters = []\n",
    "piece_dict = {}\n",
    "match_blacklist = []\n",
    "all_feature_names = set()\n",
    "\n",
    "for xml in tqdm(xml_fn):\n",
    "    \n",
    "    json_dict = {\"matches\": [], \"targets\":[], \"matched_basis\": [], \"xml\": xml}\n",
    "    \n",
    "    os.makedirs(os.path.join(os.path.dirname(xml), \"modified_matches\"), exist_ok=True)\n",
    "    \n",
    "    if not dump_json:\n",
    "        piece_dict.update({os.path.dirname(xml): {\"matches\": [], \"targets\": [], \"basis_idxs\": [], \"xml\": xml}})\n",
    "    \n",
    "    match_fn = glob.glob(os.path.join(os.path.dirname(xml), \"*.match\"))\n",
    "\n",
    "    score = pt.load_score(xml)\n",
    "    score = pt.score.merge_parts(score)\n",
    "    score = pt.score.unfold_part_maximal(score, update_ids=True)\n",
    "    \n",
    "    nid_dict = dict((n.id, i) for i, n in enumerate(score.notes_tied))\n",
    "    \n",
    "    pt.score.expand_grace_notes(score)\n",
    "    \n",
    "    basis, bf_names = pt.musicanalysis.make_note_feats(score, \"all\")\n",
    "    all_feature_names.update(bf_names)\n",
    "\n",
    "    for match in match_fn:\n",
    "\n",
    "        try:\n",
    "            performance, alignment = pt.load_match(match)\n",
    "\n",
    "            parameter_names = [\"velocity_trend\", \"beat_period\"]\n",
    "\n",
    "            pc = get_performance_codec(parameter_names)\n",
    "\n",
    "            targets, snote_ids, unique_onset_idxs = pc.encode(\n",
    "                part=score,\n",
    "                ppart=performance[0],\n",
    "                alignment=alignment,\n",
    "                return_u_onset_idx=True\n",
    "            )\n",
    "            \n",
    "            matched_subset_idxs = np.array([nid_dict[nid] for nid in snote_ids])\n",
    "            basis_matched = basis[matched_subset_idxs]\n",
    "            \n",
    "            perf_parameters.append((targets, match))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(match)\n",
    "            match_blacklist.append(match)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f7dc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_parameters(array, bp=[0, 2.5], t=[-0.1, 0.1]):\n",
    "    copy = array.copy()\n",
    "    \n",
    "    for perf, name in copy:\n",
    "        np.clip(perf[\"beat_period\"], a_max=2.5, a_min=0, out=perf[\"beat_period\"])\n",
    "        np.clip(perf[\"timing\"], a_max=0.01, a_min=-0.01, out=perf[\"timing\"])\n",
    "        \n",
    "    return copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb831fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "indices = [perf[1] for perf in perf_parameters]\n",
    "expressiveness_params = [\"beat_period\", \"timing\", \"articulation_log\", \"velocity_trend\", \"velocity_dev\"]\n",
    "descriptions = []\n",
    "dataframes = []\n",
    "\n",
    "perf_parameters = clip_parameters(perf_parameters, bp=[0, 3], t=[-1, 1])\n",
    "\n",
    "for perf in perf_parameters:\n",
    "    param_df = pd.DataFrame(perf[0], columns=expressiveness_params)\n",
    "    dataframes.append(param_df)\n",
    "    descriptions.append(param_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602a3eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb548e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bb2365",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_outliers(z_threshold=3):\n",
    "    \n",
    "    outlier_dict = {\"piece_names\": []}\n",
    "    \n",
    "    for param_name in expressiveness_params:\n",
    "\n",
    "        print(f\"{param_name} outliers:\")\n",
    "\n",
    "        param_df = pd.DataFrame(columns=[\"min\", \"max\", \"mean\"], index=indices)\n",
    "\n",
    "        param_df[\"min\"] = [desc[param_name].T[\"min\"] for desc in descriptions]\n",
    "        param_df[\"max\"] = [desc[param_name].T[\"max\"] for desc in descriptions]\n",
    "        param_df[\"mean\"] = [desc[param_name].T[\"mean\"] for desc in descriptions]\n",
    "        \n",
    "        min_mean = np.mean(param_df[\"min\"])\n",
    "        max_mean = np.mean(param_df[\"max\"])\n",
    "        mean_mean = np.mean(param_df[\"mean\"])\n",
    "        \n",
    "        min_median = np.median(param_df[\"min\"])\n",
    "        max_median = np.median(param_df[\"max\"])\n",
    "        mean_median = np.median(param_df[\"mean\"])\n",
    "\n",
    "        z_min = np.abs(stats.zscore(param_df[\"min\"]))\n",
    "        z_max = np.abs(stats.zscore(param_df[\"max\"]))\n",
    "        z_mean = np.abs(stats.zscore(param_df[\"max\"]))\n",
    "        \n",
    "        outliers = param_df[(z_min > z_threshold) | (z_max > z_threshold) | (z_mean > z_threshold)]\n",
    "        \n",
    "        print(f\"\"\"Mean values for comparison:\n",
    "                    min: {min_mean}\n",
    "                    max: {max_mean}\n",
    "                    mean: {mean_mean}\\n\"\"\")\n",
    "        \n",
    "        print(f\"\"\"Median values for comparison:\n",
    "                    min: {min_median}\n",
    "                    max: {max_median}\n",
    "                    mean: {mean_median}\\n\"\"\")\n",
    "        \n",
    "        print(f\"{outliers.to_markdown()}\\n\\n\")\n",
    "        outlier_dict[param_name] = outliers\n",
    "        outlier_dict[\"piece_names\"] += [idx for idx in outliers.index]\n",
    "        \n",
    "    return outlier_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea38405",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_dict = get_outliers(8)\n",
    "outlier_dict[\"piece_names\"] += match_blacklist\n",
    "outlier_dict[\"piece_names\"] = list(set(outlier_dict[\"piece_names\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81100590",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_dict[\"piece_names\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd231c2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "for param_name in expressiveness_params:\n",
    "    \n",
    "    if len(outlier_dict[param_name]) != 0:\n",
    "    \n",
    "        fig, ax = plt.subplots(len(outlier_dict[param_name])+1, figsize=(10, 20))\n",
    "        for i, piece_name in enumerate(outlier_dict[param_name].index):\n",
    "\n",
    "            idx = indices.index(piece_name)\n",
    "            df = dataframes[idx]\n",
    "\n",
    "            ax[i].plot(df[param_name])\n",
    "            ax[i].set(title=f\"{param_name.upper()}: {piece_name}\")\n",
    "\n",
    "        fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0827311",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_perf_params = [perf for perf in perf_parameters if perf[1] not in outlier_dict[\"piece_names\"]]\n",
    "\n",
    "print(len(perf_parameters), len(valid_perf_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaa9a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f567a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "basis_list = []\n",
    "feature_frame = pd.DataFrame(columns=feature_names)\n",
    "feature_names = None\n",
    "\n",
    "counter = 0\n",
    "for perf, perf_name in tqdm(valid_perf_params):\n",
    "    \n",
    "    xml = glob.glob(os.path.join(os.path.dirname(perf_name), \"*.musicxml\"))[0]\n",
    "    \n",
    "    score = pt.load_score(xml)\n",
    "    score = pt.score.merge_parts(score)\n",
    "    score = pt.score.unfold_part_maximal(score, update_ids=True)\n",
    "    pt.score.expand_grace_notes(score)\n",
    "\n",
    "    basis, bf_names = pt.musicanalysis.make_note_feats(score, \"all\", force_fixed_size=True)\n",
    "    basis = np.mean(basis, axis=0)\n",
    "    basis_list.append(basis)\n",
    "    \n",
    "    feature_names = bf_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b229e32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.vstack(basis_list), columns=bf_names)\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad1c86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df.corr().fillna(0)\n",
    "drop_cols = []\n",
    "\n",
    "for j, col in enumerate(corr_matrix.columns):\n",
    "    \n",
    "    if j == 0:\n",
    "        i = np.argmax(np.abs(corr_matrix[col][1:]))\n",
    "        corr = corr_matrix[col][i]\n",
    "    else:\n",
    "        i = np.argmax(np.abs(corr_matrix[col][:j:]))\n",
    "        corr = corr_matrix[col][i]\n",
    "        \n",
    "    \n",
    "    if abs(corr) > 0.85 or corr == 0:\n",
    "        drop_cols.append(col)\n",
    "    \n",
    "    print(f\"{col} has the highest correlation with {corr_matrix.index[i]}: {corr}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aaa1aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df.drop(columns=drop_cols)\n",
    "len(new_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4894d0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.heatmap(df.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37586980",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(new_df.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746f593b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7407880a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 match_paths,\n",
    "                 seq_len, param_names=[\"beat_period\", \"timing\", \"articulation_log\", \"velocity_trend\", \"velocity_dev\"],\n",
    "                 feat_names=\"all\",\n",
    "                 fixed_features=True,\n",
    "                 drop_features=[]\n",
    "                ):   \n",
    "        \n",
    "        self.data = []\n",
    "        self.target_data = None\n",
    "        self.param_names = param_names\n",
    "        self.parameter_dict = {name: [] for name in param_names}\n",
    "        \n",
    "        self.feature_size = 0\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        print(\"Processing score data...\")\n",
    "        for match_file in tqdm(match_paths):\n",
    "            \n",
    "            try:\n",
    "                \n",
    "                xml = glob.glob(os.path.join(os.path.dirname(match_file), \"*.musicxml\"))[0]\n",
    "\n",
    "                score = pt.load_score(xml)\n",
    "                score = pt.score.merge_parts(score)\n",
    "                score = pt.score.unfold_part_maximal(score, update_ids=True)\n",
    "\n",
    "                nid_dict = dict((n.id, i) for i, n in enumerate(score.notes_tied))\n",
    "\n",
    "                pt.score.expand_grace_notes(score)\n",
    "\n",
    "                basis, bf_names = pt.musicanalysis.make_note_feats(score, feat_names, force_fixed_size=fixed_features)\n",
    "\n",
    "                performance, alignment = pt.load_match(match_file)\n",
    "\n",
    "                parameter_names = param_names\n",
    "\n",
    "                pc = get_performance_codec(parameter_names)\n",
    "\n",
    "                targets, snote_ids, unique_onset_idxs = pc.encode(\n",
    "                    part=score,\n",
    "                    ppart=performance[0],\n",
    "                    alignment=alignment,\n",
    "                    return_u_onset_idx=True\n",
    "                )\n",
    "\n",
    "                matched_subset_idxs = np.array([nid_dict[nid] for nid in snote_ids])\n",
    "                basis_matched = basis[matched_subset_idxs]\n",
    "                \n",
    "                basis_matched = pd.DataFrame(basis_matched, columns=bf_names)\n",
    "                basis_matched = basis_matched.drop(columns=drop_features).to_numpy()\n",
    "                \n",
    "                # clipping outliers\n",
    "                \n",
    "                np.clip(targets[\"beat_period\"], a_min=0, a_max=3, out=targets[\"beat_period\"])\n",
    "                np.clip(targets[\"timing\"], a_min=-1, a_max=1, out=targets[\"timing\"])\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(match_file)\n",
    "                print(e)\n",
    "                continue\n",
    "            \n",
    "            padding_len = len(targets) % seq_len\n",
    "            \n",
    "            for name in param_names:\n",
    "                new_targets = targets[name]\n",
    "                t_padding_array = np.zeros(shape=seq_len - padding_len)\n",
    "                new_targets = np.concatenate((new_targets, t_padding_array))\n",
    "                new_targets = np.split(new_targets, len(new_targets) / seq_len)\n",
    "                \n",
    "                self.parameter_dict[name] += [target for target in new_targets]\n",
    "            \n",
    "            bm_padding_array = np.zeros(shape=(seq_len - padding_len, basis_matched.shape[1]))\n",
    "            new_basis = np.vstack((basis_matched, bm_padding_array))\n",
    "            new_basis = np.split(new_basis, len(new_basis) / seq_len)\n",
    "            \n",
    "            self.data += [basis for basis in new_basis]\n",
    "            \n",
    "        max_features = max([basis.shape[1] for basis in self.data])\n",
    "        self.feature_size = max_features\n",
    "        \n",
    "        for i, basis in enumerate(self.data):\n",
    "            if basis.shape[1] < max_features:\n",
    "                \n",
    "                difference = max_features - basis.shape[1]\n",
    "                \n",
    "                self.data[i] = np.hstack((basis, np.zeros(shape=(seq_len, difference))))\n",
    "            \n",
    "    def choose_parameter(self, parameter_name):\n",
    "        self.target_data = self.parameter_dict[parameter_name]\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        x = self.data[idx]\n",
    "        y = self.target_data[idx]\n",
    "        \n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3eeb0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = glob.glob(os.path.join(\"asap-dataset\\Bach\", \"**\", \"*.match\"), recursive=True)\n",
    "\n",
    "custom_dataset = MyDataset(matches, seq_len=50, drop_features=drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05822792",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(custom_dataset, \"my_data.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787019a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device).to(x.dtype)\n",
    "        # Initialize cell state\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device).to(x.dtype)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(self.relu(out[:, -1, :]))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab19cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device).to(x.dtype)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.gru(x, h0)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(self.relu(out[:, -1, :]))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d83edf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMSeq2Seq(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMSeq2Seq, self).__init__()\n",
    "        \n",
    "        output_size = 1\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.encoder = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.decoder = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, src):\n",
    "        batch_size = src.size(0)\n",
    "\n",
    "        # Encoder\n",
    "        encoder_output, (hidden, cell) = self.encoder(src)\n",
    "\n",
    "        # Decoder initialization with encoder's last hidden and cell state\n",
    "        decoder_input = torch.zeros(batch_size, 1, self.hidden_size)\n",
    "\n",
    "        outputs = []\n",
    "        for i in range(encoder_output.size(1)):\n",
    "            decoder_output, (hidden, cell) = self.decoder(decoder_input, (hidden, cell))\n",
    "            output = self.output_layer(decoder_output)\n",
    "            outputs.append(output)\n",
    "            decoder_input = decoder_output\n",
    "\n",
    "        return torch.cat(outputs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82512659",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.query = nn.Linear(hidden_size, hidden_size)\n",
    "        self.key = nn.Linear(hidden_size, hidden_size)\n",
    "        self.value = nn.Linear(hidden_size, hidden_size)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        q = self.query(inputs)\n",
    "        k = self.key(inputs)\n",
    "        v = self.value(inputs)\n",
    "        \n",
    "        attention_scores = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.hidden_size, dtype=torch.float32))\n",
    "        attention_probs = self.softmax(attention_scores)\n",
    "        attention_output = torch.matmul(attention_probs, v)\n",
    "        \n",
    "        return attention_output\n",
    "\n",
    "class AttentiveLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(AttentiveLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.self_attention = SelfAttention(hidden_size)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # LSTM layer\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # Self-Attention layer\n",
    "        self_attended = self.self_attention(lstm_out[:, -1, :])\n",
    "        \n",
    "        # Apply fully connected layer\n",
    "        output = self.fc(self_attended)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add81e02",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "NUM_EPOCHS = 50\n",
    "NUM_LAYERS = 1\n",
    "HIDDEN_SIZE = 264\n",
    "\n",
    "models = []\n",
    "param_names = [\"beat_period\", \"timing\", \"articulation_log\", \"velocity_trend\", \"velocity_dev\"]\n",
    "fig, ax = plt.subplots(5, figsize=(10, 15))\n",
    "\n",
    "# train model for each parameter\n",
    "for i, name in enumerate(param_names):\n",
    "    \n",
    "    print(f\"Training model for {name}\\n\")\n",
    "    \n",
    "    custom_dataset.choose_parameter(name)\n",
    "\n",
    "    rng = np.random.default_rng()\n",
    "    all_indices = list(range(len(custom_dataset)))\n",
    "    rng.shuffle(all_indices)\n",
    "\n",
    "    test_indices = all_indices[:int(len(all_indices) * 0.2)]\n",
    "    val_indices = all_indices[int(len(all_indices) * 0.2):int(len(all_indices) * 0.3)]\n",
    "    train_indices = all_indices[int(len(all_indices) * 0.3):]\n",
    "    \n",
    "    # initialize model and dataloaders\n",
    "    my_model = LSTMSeq2Seq(input_size=custom_dataset.feature_size, hidden_size=HIDDEN_SIZE, output_size=custom_dataset.seq_len)\n",
    "\n",
    "    train_set = Subset(custom_dataset, indices=train_indices)\n",
    "    test_set = Subset(custom_dataset, indices=test_indices)\n",
    "    val_set = Subset(custom_dataset, indices=val_indices)\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n",
    "    val_loader = DataLoader(val_set, batch_size=64, shuffle=False)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    learning_rate = 1e-4\n",
    "    optimizer = optim.Adam(my_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    my_model.to(device)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = [np.inf]\n",
    "    best_model = None\n",
    "    best_val_loss = np.inf\n",
    "\n",
    "    # Train/Validation loop\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "        # Training\n",
    "        total_train_loss = 0\n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            inputs, targets = inputs.float(), targets.float()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = my_model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), targets.squeeze())  # Assuming the output and target shapes are compatible\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        # Validating\n",
    "        total_val_loss = 0\n",
    "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            inputs, targets = inputs.float(), targets.float()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = my_model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), targets.squeeze())\n",
    "\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "        if total_val_loss / len(val_loader) < best_val_loss:\n",
    "            best_model = my_model.state_dict()\n",
    "\n",
    "        if len(val_losses) > 10 and min(val_losses) not in val_losses[-10:]:\n",
    "            print(\"Validation loss is increasing, stopping early...\")\n",
    "            break\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{NUM_EPOCHS}], Average train Loss: {total_train_loss / len(train_loader):.4f}, Average validation Loss: {total_val_loss / len(val_loader):.4f}\")\n",
    "        train_losses.append(total_train_loss / len(train_loader))\n",
    "        val_losses.append(total_val_loss / len(val_loader))\n",
    "\n",
    "    print(\"Training finished\\n\")\n",
    "    torch.save(best_model, f\"best_model_{name}.pt\")\n",
    "\n",
    "    # Testing\n",
    "    model = LSTMSeq2Seq(input_size=custom_dataset.feature_size, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS, output_size=custom_dataset.seq_len)\n",
    "    model.load_state_dict(best_model)\n",
    "    \n",
    "    total_test_loss = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        inputs, targets = inputs.float(), targets.float()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = my_model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), targets.squeeze())\n",
    "\n",
    "        total_test_loss += loss.item()\n",
    "\n",
    "    print(f\"Average loss on test set: {total_test_loss / len(test_loader)}\\n\\n\")\n",
    "    models.append(model)\n",
    "    \n",
    "    ax[i].plot(train_losses)\n",
    "    ax[i].plot(val_losses)\n",
    "    \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d05acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = Subset(custom_dataset, list(range(20)))\n",
    "new_dataloader = DataLoader(new_dataset, shuffle=False, batch_size=1)\n",
    "\n",
    "param_names = [\"beat_period\", \"timing\", \"articulation_log\", \"velocity_trend\", \"velocity_dev\"]\n",
    "preds = {name: [] for name in param_names}\n",
    "ground_truths = {name: [] for name in param_names}\n",
    "\n",
    "for inputs, targets in new_dataloader:\n",
    "    inputs, targets = inputs.to(device), targets.to(device)\n",
    "    inputs, targets = inputs.float(), targets.float()\n",
    "    \n",
    "    for model, name in zip(models, param_names):\n",
    "        model.to(device)\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        preds[name] += outputs.tolist()[0]\n",
    "\n",
    "zipped_list = [(a, b, c, d, e) for a, b, c, d, e in zip(preds[\"beat_period\"], preds[\"timing\"], preds[\"articulation_log\"], preds[\"velocity_trend\"], preds[\"velocity_dev\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9993a0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_perf = np.array(zipped_list, dtype=perf_parameters[0][0].dtype)\n",
    "test_perf[\"beat_period\"] = np.abs(test_perf[\"beat_period\"])\n",
    "test_perf[\"velocity_trend\"] = np.abs(test_perf[\"velocity_trend\"])\n",
    "test_perf[\"velocity_dev\"] = np.abs(test_perf[\"velocity_dev\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5ef930",
   "metadata": {},
   "outputs": [],
   "source": [
    "xml = glob.glob(os.path.join(\"asap-dataset\", \"Bach\", \"Fugue\", \"bwv_846\", \"*.musicxml\"))[0]\n",
    "\n",
    "score = pt.load_score(xml)\n",
    "score = pt.score.merge_parts(score)\n",
    "score = pt.score.unfold_part_maximal(score, update_ids=True)\n",
    "pt.score.expand_grace_notes(score)\n",
    "\n",
    "pc = get_performance_codec(param_names)\n",
    "\n",
    "result = pc.decode(score, test_perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffbf530",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4bd386",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt.save_performance_midi(result, \"test_midi.mid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7829ea04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rendering(name):\n",
    "    custom_dataset.choose_parameter(name)\n",
    "\n",
    "    new_dataset = Subset(custom_dataset, list(range(20)))\n",
    "    new_dataloader = DataLoader(new_dataset, shuffle=False, batch_size=1)\n",
    "\n",
    "    ground_truths = []\n",
    "\n",
    "    for inputs, targets in new_dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        inputs, targets = inputs.float(), targets.float()\n",
    "\n",
    "        ground_truths += targets.tolist()[0]\n",
    "\n",
    "    plt.plot(ground_truths)\n",
    "    plt.plot(test_perf[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20477f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rendering(\"beat_period\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df9b78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rendering(\"timing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d21cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rendering(\"articulation_log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc9ec42",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rendering(\"velocity_trend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f478e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rendering(\"velocity_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e667eca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c360494",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
